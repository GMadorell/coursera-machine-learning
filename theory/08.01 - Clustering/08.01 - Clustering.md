# Unsupervised Learning: Introduction #
Clustering will be our first unsupervised algorithm.  

In clustering, we will no longer have labels for the data. We will make the algorithm find some structure in the data, clusters.

Applications: 

* Market segmentation.
* Social network analysis.
* Organize computing clusters.
* Astronomical data analysis.


# K-Means Algorithm #
Iterative algorithm for grouping data in clusters:

Input = number of clusters and unlabeled training set.

1. Randomly initialize cluster centroids.
2. Assign each data point to its closest centroid.
3. Move each centroid to the mean of the points assigned to them.
4. Iterate 2 and 3 until no major change in step 3 happens.

A more formal description is given in \cref{alg:kmeans}:

\begin{algorithm}
\caption{Kmeans algorithm}\label{alg:kmeans}
\begin{algorithmic}[1]
\Function{Kmeans}{$K, x$} \Comment{K = number of clusters, x = unlabeled dataset}
    \State Randomly initialize $K$ cluster centroids $\mu_1, \mu_2, ... , \mu_K \in \mathbb{R}$.
    \While{not convergence}
        \For{$i$ = 1 to $m$} \Comment{m is the length of the dataset}
            \State $c^{(i)} \gets$ index of cluster centroid closest to $x^{(i)}$
        \EndFor 
        \For{$k$ = 1 to $K$} 
            \State $\mu_k \gets$ average (mean) of points assigned to cluster $k$
        \EndFor
    \EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}

To find out which cluster is the closest one, we can use any distance (such as the euclidian one).

Convergence is declared once there is no change between two consecutive runs.

If a cluster ends with no points assigned to it, simply eliminate it or reinitialize it randomly if the amount of clusters can't be diminished at all.


# Optimization Objective #
The algorithms we've seen all have a cost function to optimize. Turns out, Kmeans also has a cost function. Let's remember some of the kmean notation first:

* $c^{(i)} = \text{index of cluster to which example } x^{(i)} \text{ is currently assigned}$
* $\mu_k = \text{cluster centroid  } k (\mu_k \in \mathbb{R}^n)$
* $\mu_c(i) = \text{cluster centroid of cluster to which example  } x^{(i)} \text{  has been assigned}$

Given that, we can now see that kmeans is minimizing \cref{eq:kmeans_cost_function}

\begin{equation} \label{eq:kmeans_cost_function}
J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_K) = \frac{1}{m} \sum_{i=1}^{m} \norm{x^{(i)} - \mu_c(i)}^2
\end{equation}

\cref{eq:kmeans_cost_function} is sometimes called the distortion formula.


# Random Initialization #
We need to make sure that the kmeans algorithm evades local optimal. Some things that help with that are:

* $K < m$: number of clusters < number of data points.
* Randomly choosing $K$ training examples instead of choosing freely random points for cluster initialization.

Even using that, it might end up in a local optima. We can initialize kmeans several times to solve that as shown in \cref{alg:kmeans_no_local_optima}.

\begin{algorithm}
\caption{Kmeans solving local optima problem}\label{alg:kmeans_no_local_optima}
\begin{algorithmic}[1]
\Function{Kmeans}{$K, x, repetitions$} \Comment{K = number of clusters, x = unlabeled dataset}
    \For{$i$ = 1 to repetitions}
        \State Run normal K-means.
    \EndFor
    \State \Return the instance that gets the lowest cost function.
\EndFunction
\end{algorithmic}
\end{algorithm}


# Choosing the number of clusters #
The most common thing to do is to explore that data in order to know the optimum amount of clusters. \smallskip

A method sometimes used is called Elbow number (\cref{fig:elbow_method}), in which you pick the point in which the cost function no longer decreases very fast. This can only be applied in certain cases in which that value is very clear, which might not be always as shown in the right graph of \cref{fig:elbow_method}. \bigskip

\stdfig{elbow_method}{Graphs indicating when to use the elbow method to select the optimal amount of clusters.}

Sometimes Kmeans is used to get clusters to use for some later purpose. We might exploit that to select the amount of clusters so that it performs well for that purpose.

For example, if we are clustering t-shirts sizes, to find out the amount of t-shirt sizes we have to produce, as seen in \cref{fig:t_shirt_clustering}. We can think about it from the t-shirt business point of view, and we might conclude that having less t-shirt sizes might be cheaper, so we would prefer an small amount of clusters.

\stdfig{t_shirt_clustering}{Graphs showing different cluster amounts for the t-shirt industry.}
    
