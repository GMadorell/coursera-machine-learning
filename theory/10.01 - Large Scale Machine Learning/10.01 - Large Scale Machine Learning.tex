
\RequirePackage[l2tabu, orthodox]{nag} % Complaints if syntax isn't right.
\documentclass[10pt]{extarticle}

\usepackage[utf8]{inputenc}
\usepackage{extsizes} % Allow for more sizes such as 14pt or 17pt on document class.
\usepackage{mathtools} % Allows conditional math expressions, etc.
\usepackage{amsfonts} % Allows \mathbb{R}, which is the real numbers symbol.
\usepackage[export]{adjustbox} % Wraps \includegraphics with more keys (options).


% For writing pseudocode snippets: http://ctan.mackichan.com/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{algorithm}
\usepackage{algpseudocode}




\usepackage[top=3.5cm, bottom=3.5cm, left=3.5cm, right=3.5cm]{geometry}

\usepackage{microtype} % Makes document more readable by optimizing space between letters.

\usepackage{units} % Adds \nicefrac{1}{2}: (Â½), \unit[5]{m}{s}: 5 m/s.

\usepackage[colorlinks=false, pdfborder={0 0 0}]{hyperref} % Allow clicking references and the table of contents in pdfs.
\usepackage{cleveref} % Adds fig/formula etc. before references (use \cref).

%%% New commands.

% Don't output references in case they're empty - http://tex.stackexchange.com/questions/74476/how-to-avoid-empty-thebibliography-environment-bibtex-if-there-are-no-refere

\let\myBib\thebibliography
\let\endmyBib\endthebibliography

\renewcommand\thebibliography[1]{\ifx\relax#1\relax\else\myBib{#1}\fi}


% Norm -> ||something||
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Stdfig -> Used as \stdfig{width}{label_name}{caption}
% Requires: image called 'caption' in img folder.
% Output: A figure with the given width, labeled as 'fig:label_name'

\newcommand{\stdfig}[3]{
    \begin{figure}
    \centering
    \includegraphics[width = #1]{img/#2.eps}
    \caption{#3}
    \label{fig:#2}
    \end{figure}
}



\begin{document} 



%% Front page.
\title{10.01 - Large Scale Machine Learning}

    
    \date{}
    

    

    \maketitle

\newpage
%% Abstract page.


%% Table of contents page.



%% Body start.
\section{Learning With Large
Datasets}\label{learning-with-large-datasets}

One of the reasons why algorithms are getting better is to simply get
more data.

\begin{quote}
``It's not who has the best algorithm that wins. It's who has the most
data.''
\end{quote}

Learning with large datasets comes with a price: computational time
(they run very slow). A solution for that is to simply take a randomized
subset of the whole data. We can plot the learning curves to see if that
works. For example, in \cref{fig:learning_curves_high_variance}, we can
see that getting more data is likely to help when we have high variance.
On the other hand, in \cref{fig:learning_curves_high_bias} we can see
that, when we have high bias, the amount of data doesn't matter after a
certain point. In this last case, we can think about adding more
features/more hidden layer units.

\stdfig{7cm}{learning_curves_high_variance}{Learning curves with high variance}

\stdfig{7cm}{learning_curves_high_bias}{Learning curves with high bias}

\section{Stochastic Gradient Descent}\label{stochastic-gradient-descent}

This algorithm is a modification to the standard gradient descent
(called ``Batch gradient descent'', as it uses all the data on every
iteration) that allows us to train better on large scale datasets.
\smallskip

In this new algorithm, we modify $J_{train}$ to look like this (which is
the same as the batch one but written in a different way):

\begin{equation}
cost(\theta, (x^{(i)}, y^{(i)})) = \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)}) ^2
\end{equation}

\begin{equation}
J_{train}(\theta) = \frac{1}{m} \sum_{i=1}^m cost(\theta,(x^{(i)}, y^{(i)})) 
\end{equation}

We can see a description of the algorithm in
\cref{alg:stochastic_gradient_descent}.

It basically goes example by example, each time fitting the $\theta$
values a little bit better. What batch did was to have a sumation for
every example in the data, here we do the descent example by example.

In conclusion, stochastic can run much faster in huge datasets at the
cost of giving an approximation of the local minimum instead of the
exact local minimum.

\begin{algorithm}
    \caption{Stochastic gradient descent}
    \label{alg:stochastic_gradient_descent}
\begin{algorithmic}[1]
\State Randomly shuffle dataset.
\State \Repeat
    \For{$i\gets 1, m$}  \Comment{$m$ is the amount of examples in the dataset}
        \For{$j\gets 1, n$}  \Comment{$n$ is the amount of $\theta$ to fit}
            \State $\theta_j := \theta_j - \alpha (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$
        \EndFor
    \EndFor
\Until{amount of times}  \Comment{Hyperparameter: if dataset is large 1 may be enough}
\end{algorithmic}
\end{algorithm}

\section{Mini-Batch Gradient Descent}\label{mini-batch-gradient-descent}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Batch gradient descent = use all $m$ examples in each iteration.
\item
  Stochastic gradient descent = use 1 example in each iteration.
\item
  Mini-Batch gradient descent = use $b$ examples in each iteration.
\end{itemize}

$b$ is the mini-batch size, with a typical size of 2-100. It might be
faster if the implementation is well vectorized.

\section{Stochastic Gradient Descent
Convergence}\label{stochastic-gradient-descent-convergence}

Back when we were using batch, we could plot $J_{train}(\theta)$ as a
function of the number of iterations of gradient descent and check
whether it was always decreasing or not.

In stochastic, we can't do that, though, as in order to plot
$J_{train}(\theta)$ we need to iterate over all $m$. What we can do is,
during learning, compute the $cost$ of the actual example before
updating $\theta$. Then, every 1000 iterations (say), we can plot the
$cost$ averaged over the last 1000 examples. This graphic can be noisy
and not decrease in some points, as we can see in
\cref{fig:stochastic_cost_plot_ideal}. If we see in the graph that the
line isn't decreasing, we could try to increase the amount of iteration
we average (\cref{fig:stochastic_cost_plot_noisy}).

\stdfig{7cm}{stochastic_cost_plot_ideal}{Ideal plot of the stochastic gradient descent}

\stdfig{7cm}{stochastic_cost_plot_noisy}{Noisy plot (blue) vs plot with a larger number of iterations (red) of the stochastic gradient descent}

\subsection{Learning rate tunning}\label{learning-rate-tunning}

Stochastic gradient descent gets near the local minimum and then wanders
around, never converging. If we want to try to make it converge, we can
slowly decrease $\alpha$ over time, for example with
\cref{eq:learning_rate_decrease}. This comes at the cost of tunning
extra hyperparameters.

\begin{equation} \label{eq:learning_rate_decrease}
\alpha = \frac{const1}{iterationNumber + const2}
\end{equation}

\section{Online learning}\label{online-learning}

This style of learning happens when we have a constant stream of data to
learn from, ie a website.

An example of that would be a shipping service website where user comes,
specifies origin and destination, you offer to ship their package for
some asking price, and users sometimes choose to use your shipping
service ($y$ = 1), sometimes not ($y$ = 0).

Features $x$ capture properties of user, of origin/destination and
asking price. We want to learn $p(y=1|x;\theta)$ to optimize price.
\smallskip

We will use \cref{alg:online_learning} for that. Basically, it's very
similar to stochastic gradient descent but we discard the user data
after using it. A nice property of this algorithm is that it adapts if
your users change over time.

\begin{algorithm}
    \caption{Online Learning}
    \label{alg:online_learning}
\begin{algorithmic}[1]
\While{forever}
    \State Wait until a new example comes in from the stream.
    \State Get the example $x$ and $y$.
    \For{$j\gets 1, n$}  \Comment{$n$ is the amount of $\theta$ to fit}
        \State $\theta_j := \theta_j - \alpha (h_\theta(x) - y) x_j$
    \EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}

\section{Map Reduce and Data
Parallelism}\label{map-reduce-and-data-parallelism}

Sometimes we just have so much data that we can't train using a single
computer. We can use Map Reduce in order to solve that problem.
\smallskip

The idea is: imagine that we have 4 machines and are using batch
gradient descent. We can have each of the machines compute
$\nicefrac{1}{4}$ of the sum needed for a gradient descent step and then
collaborate with the result of the partial sums.

We can look at that visually in \cref{fig:map_reduce_scheme}. \smallskip

\stdfig{10cm}{map_reduce_scheme}{Scheme of a map reduce solution}

Many learning algorithms can be expressed as computing sums of cuntions
over the training set. This are the types of algorithms that can be run
with map-reduce. A good place to look for is the sums over all the
dataset ($\sum_{i=1}^m$), which can be parallelized. \smallskip

Map reduce can even be useful in a single computer. We can simply send
each job to a core instead of another computer.


%% Body end.

%% Bibliography.

\clearpage



    \nocite{*}

\bibliographystyle{plain}
\bibliography{references}

\end{document}