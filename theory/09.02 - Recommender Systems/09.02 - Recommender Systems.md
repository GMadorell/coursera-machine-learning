# Problem Formulation #
Example: Predicting movie ratings.

Imagine a company where users rate movies using zero to five stars.

Notation:

* $n_u$ = number of users
* $n_m$ = number of movies
* $r(i, j)$ = 1 if user $j$ has rated movie $i$
* $y^{(i,j)}$ = rating given by user $j$ to movie $i$ (defined only if $r(i, j)$ = 1)

The problem is: given this dataset, to look through all the missing ratings on the dataset and fill them (\cref{fig:movie_ratings_table}).

\stdfig{9cm}{movie_ratings_table}{Movie ratings table}


# Content Based Recommendations #
How do we predict the "?" in the movies table (\cref{fig:movie_ratings_table})? 

Let's guess that we have two additional features in the table: one that measures the amount of romance a movie has and another one that measures the amount of action (\cref{fig:movie_ratings_table_with_features}). 

\stdfig{11cm}{movie_ratings_table_with_features}{Movie ratings table with features}

If we then add the interceptor term $x_0 = 1$, we have a feature vector for each movie ($x^{(1)} = [1, \, 0.9, \,  0]^T$, for example). \bigskip

The idea is to use linear regression for each different user and predict the missing parameters with that. More formally: for each user $j$, learn a parameter $\theta^{(j)} \in \mathbb{R}^3$. Predict user $j$ as rating movie $i$ with $(\theta^{(j)})^Tx^{(i)}$ stars. \smallskip

For example, in \cref{fig:movie_ratings_table_with_features}, if we wanted to predict "Cute pupies of love" for "Alice", and we already got $\theta$, we would do: $(\theta^{(1)})^Tx^{(3)}$. \smallskip

## Optimization Objective ##
We can describe this intention (learn $\theta$ for the user $j$) with the formula:

$$ \min_{\theta^{(j)}}  \frac{1}{2} \sum_{i:r(i, j)=1} ((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^n(\theta_k^{(j)})^2 $$

Where $\sum_{i:r(i, j)=1} $ is a for loop with all the rated movies by that user. \smallskip

To learn all the $\theta$, we can do:

$$ \min_{\theta^{(1)}, \dots, \theta^{(n_u)}}  \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i, j)=1} ((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2 $$

Which is very, very similar to the standard linear regression equation (only filtering the unknows votes out).

### Gradient descent update ###
$$ \theta_k^{(j)} := \theta_k^{(j)} - \alpha \sum_{i:r(i, j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)}\right) x_k^{(i)} \,\, \text{(for $k$ = 0)} $$

$$ \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left( \sum_{i:r(i, j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)}\right) x_k^{(i)} + \lambda\theta_k^{(j)} \right) \,\, \text{(for $k \neq$ 0)} $$


# Collaborative Filtering #
This algorithm has the characteristic of learning the features it needs by himself. \smallskip

In the last example (\cref{fig:movie_ratings_table_with_features}) we had some features. Extracting those features is quite time consuming, though. \smallskip

In this new algorithm, we won't have the features (\cref{fig:movie_ratings_table_unknown_features}). Let's imagine that the users tells us whether they like romance and action movies (the $\theta$). Then, it's possible to infer the values of $x_1$ and $x_2$. \smallskip 

\stdfig{12cm}{movie_ratings_table_unknown_features}{Move ratings with unknown features}

We can exploit that we know the ratings of the users and their preferences to know which value to assign to each movie feature. For example, if a user says that he likes action movies and he rates a movie very high, we can assume that that movie will have action.

## Optimization Algorithm ##
Given $\theta^{(1)}, \dots, \theta^{(n_u)}$, to learn $x^{(i)}$:

$$ \min_{x^{(i)}}  \frac{1}{2} \sum_{i:r(i, j)=1} ((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^n(x_k^{(i)})^2 $$

Which reads as: choosing the features so that the predicted value will be similar to the actual value that we observe on the rating of the users. \smallskip

We can then compute this for all the movies. Given $\theta^{(1)}, \dots, \theta^{(n_u)}$, to learn $x^{(1)}, \dots, x^{(n_m)}$:

$$ \min_{x^{(1)}, \dots, x^{(n_m)}}  \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i, j)=1} ((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n(x_k^{(i)})^2 $$

### Gradient descend update ###
$$ x_k^{(i)} := x_k^{(i)} - \alpha \sum_{j:r(i, j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)}\right) \theta_k^{(j)} \,\, \text{(for $k$ = 0)} $$

$$ x_k^{(i)} := x_k^{(i)} - \alpha \left( \sum_{j:r(i, j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)}\right) \theta_k^{(j)} + \lambda x_k^{(i)} \right) \,\, \text{(for $k \neq$ 0)} $$

## How to continue ##
We've seen in content based filtering that given $x^{(1)}, \dots, x^{(n_m)}$ we can estimate $\theta^{(1)}, \dots, \theta^{(n_u)}$. \smallskip

In collaborative filtering, given $\theta^{(1)}, \dots, \theta^{(n_u)}$, we can estimate $x^{(1)}, \dots, x^{(n_m)}$. \bigskip

So, this is the egg-chicken problem. What we can do, therefore, is randomly guess $\theta$ to learn $x$ (features). We can then use the $x$ to learn $\theta$, etc. 


# Collaborative Filtering Algorithm #
We've talked about the ideas of how if you're given features for movies, we can learn parameters $\theta$ for users. We've also seen how if we're given parameters for the users we can use that to extract features for the movies. We'll now find an algorithm to solve that problem. \bigskip

We could to the chicken-egg back and forth optimization algorithm, but there is an algorithm that optimizes both $x$ and $\theta$ at the same time, so it's better. \smallskip

The cost function is:

$$ J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)}) = \frac{1}{2} \sum_{(i,j):r(i, j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)}\right)^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n(x_k^{(i)})^2  + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2$$

Which is basically merging both cost functions (collaborative and content based filtering) into one, taking advantage that the less squares sumation is the same in both and then adding the respective regularization term.

We also suppress the interceptor term $x_1 = 1$, as the algorithm will learn it by himself if it's really needed.

## Gradient descent  ##
The same as before for the $\theta$ and $x$ respectively.

## Detailed algorithm ##

We can find a more detailed explanation in \cref{alg:collaborative_filtering}.

\begin{algorithm}
\caption{Collaborative filtering algorithm} \label{alg:collaborative_filtering}
\begin{algorithmic}[1]
\State Initialize $x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)}$ to small random values.
\State Minimize $J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)}, \dots, \theta^{(n_u)})$ using gradient descent (or an advanced optimization algorithm).

For every $j = 1, \dots, n_u, i = 1, \dots, n_m$:

$ x_k^{(i)} := x_k^{(i)} - \alpha \left( \sum_{j:r(i, j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)}\right) \theta_k^{(j)} + \lambda x_k^{(i)} \right) $

$ \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left( \sum_{i:r(i, j)=1} \left( (\theta^{(j)})^Tx^{(i)} - y^{(i,j)}\right) x_k^{(i)} + \lambda\theta_k^{(j)} \right) $

\State For a user with parameters $\theta$ and a movie with (learned) features $x$, predict a star rating of $\theta^Tx$.

\end{algorithmic}
\end{algorithm}


# Vectorization: Low Rank Matrix Factorization #
We'll see an alternative algorithm for solving the collaborative filtering problem. \bigskip

Take the dataset we have (\cref{fig:movie_ratings_table}), and put it directly onto a matrix called $Y$. Then, we can stack the $x$ vectors and the $\theta$ vectors in two matrices, $X$ and $\Theta$. Afterwards, we can compute the prediction of the ratings simply by doing $X\Theta^T$. All this can be seen in \cref{fig:low_rank_matrix_factorization}.

\stdfig{10cm}{low_rank_matrix_factorization}{Low rank matrix factorization}


## Finding related movies ##
For each product $i$, we learn a feature vector $x^{(i)} \in \mathbb{R}^n$.

How to find 5 movies $j$ most related to movie $i$?  

Find the 5 movies $j$ with the smallest $\norm{x^{(i)} - x^{(j)}}$.


# Implementation detail: Mean Normalization #
The anterior algorithm works fine, except when there is a user that hasn't rated any movie. To solve that, after we already have $Y$, we calculate the mean of every movie (only of the existing ratings). Afterwards, we substract the mean of a movie to every rating of that movie. We then use this new modified $Y$ matrix to learn. 

Then we make a prediction, we need to add the mean again.